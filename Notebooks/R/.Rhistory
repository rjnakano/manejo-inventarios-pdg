i = which.min(m2$cptable[,4])
lim_inferior = m2$cptable[i,4] - m2$cptable[i,5]
lim_superior = m2$cptable[i,4] + m2$cptable[i,5]
print(paste("Intervalo de error: [", lim_inferior, ", ", lim_superior, "]"))
indices = which((m2$cptable[,4] >= lim_inferior) & (m2$cptable[,4] <= lim_superior))
indice_corte = min(indices)
umbral_corte_cp = m2$cptable[indice_corte,1]
umbral_corte_cp
m2 = prune(m2, cp=umbral_corte_cp)
m2$cptable
rpart.plot(m2)
predicciones <- predict(m2, newdata=dataTest, type='class')
confusionMatrix(predicciones, dataTest$Revenue)
base2 = base
base2$Administrative_Ranges = NULL
base2$Informational_Ranges = NULL
base2$ProductRelated_Ranges = NULL
base2$Administrative_Duration_Ranges = NULL
base2$Informational_Duration_Ranges = NULL
base2$ProductRelated_Duration_Ranges = NULL
base2$ExitRates_Ranges = NULL
base2$BounceRates_Ranges = NULL
base2$PageValues_Ranges = NULL
seed = 123
set.seed(seed)
trainIndex <- createDataPartition(base2$Revenue, p = .75, list = FALSE, times = 1)
dataTrain <- base2[trainIndex,]
dataTest <-  base2[-trainIndex,]
dim(dataTrain)
dim(dataTest)
trainControl=trainControl(method="cv", number=5)
set.seed(seed)
model_arbol1 <- train(Revenue~., dataTrain,
method='rpart',
trControl = trainControl)
model_arbol1
plot(model_arbol1)
m <- model_arbol1$finalModel
rpart.plot(m, cex = 0.7)
m$parms
m$control
m$variable.importance
plot(m$variable.importance)
############################################################################
# Modelo sin poda                                                          #
############################################################################
set.seed(seed) # el valor de la semilla, en sí, no es importante, solo que se utilice siempre el mismo
grid <- data.frame(cp=c(0)) #Fijamos en 0 el parámetro de complejidad eliminando la poda controlada por Caret
modelo_arbol2 <- train(Revenue~., data = dataTrain, method = "rpart",
tuneGrid=grid,
control = rpart.control(
minsplit = 2, #min instancias para intentar split de un nodo, por defecto es 20
minbucket = 1, #min instancias en un nodo hoja terminal, por defecto minsplit/3
maxdepth = 10 #max depth, por defecto es 30
))
rpart.plot(modelo_arbol2$finalModel)
############################################################################
# Modelo con post-poda                                                     #
############################################################################
set.seed(seed) # el valor de la semilla, en sí, no es importante, solo que se utilice siempre el mismo
m2 <- rpart(Revenue~., data = dataTrain,
control = rpart.control(
cp=0, # sin prepoda de complejidad
minsplit = 2, #min instancias para intentar split de un nodo, por defecto es 20
minbucket = 1, #min instancias en un nodo hoja terminal, por defecto minsplit/3
maxdepth = 10, #max depth, por defecto es 30
xval=10 # rpart hace validación cruzada sobre el set de entranamiento para estimar los errores relativos
))
head(m2$cptable, 20)
# Analicemos los valores del error relativo con respecto al valor de CP. Entre mayor el valor del CP, menor la complejidad del árbol
ggplot(data.frame(m2$cptable), aes(x=CP, y=rel.error)) +
geom_point(size=2) +
scale_x_continuous(limits = c(0, 0.25), breaks = seq(0,0.25, 0.02)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2))
plotcp(m2)
# Intervalo de error
i = which.min(m2$cptable[,4])
lim_inferior = m2$cptable[i,4] - m2$cptable[i,5]
lim_superior = m2$cptable[i,4] + m2$cptable[i,5]
print(paste("Intervalo de error: [", lim_inferior, ", ", lim_superior, "]"))
indices = which((m2$cptable[,4] >= lim_inferior) & (m2$cptable[,4] <= lim_superior))
indice_corte = min(indices)
umbral_corte_cp = m2$cptable[indice_corte,1]
umbral_corte_cp
m2 = prune(m2, cp=umbral_corte_cp)
m2$cptable
rpart.plot(m2)
predicciones <- predict(m2, newdata=dataTest, type='class')
confusionMatrix(predicciones, dataTest$Revenue)
ggplot(data, aes(x=Month)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=OperatingSystems)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=Browser)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=Administrative_Duration_Ranges, y=Informational_Duration_Ranges, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Administrative, y=Administrative_Duration, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=ProductRelated_Duration, y=Administrative_Duration, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=ProductRelated_Duration, y=BounceRates, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=ProductRelated_Duration, y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Administrative_Ranges , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Administrative , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
(base, aes(x=Informational , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Informational , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=ProductRelated , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Administrative_Duration , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Informational_Duration , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=ProductRelated_Duration , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=ExitRates , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=BounceRates , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Month)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Administrative)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Informational)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=ProductRelated)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Administrative_Duration)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Informational_Duration)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=ProductRelated_Duration)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=ExitRates)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=BounceRates)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=PageValues)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Weekend)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=SpecialDay)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
table(base$Revenue)
prop.table(table(base$Revenue))
table(data$Revenue)
tabla = prop.table(table(data$Revenue))
cat(sprintf("htmlesc<<Proporciones de compra: %s>>\n",tabla))
print(tabla)
table(data$Revenue)
cat(sprintf("htmlesc<<Proporciones de compra (Revenue):>>\n"))
print(prop.table(table(data$Revenue)))
obj = prop.table(table(data$Revenue))
msg = "Proporciones de compra (Revenue):"
# cat(sprintf("htmlesc<<p,Proporciones de compra (Revenue):>>\n"))
print(paste(msg,obj))
# cat(sprintf("htmlesc<<p,Proporciones de compra (Revenue):>>\n"))
print(msg)
print(obj)
ggplot(data, aes(x=Month)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=OperatingSystems)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=Browser)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=Region)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=TrafficType)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=VisitorType)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=Administrative_Ranges)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=Informational_Ranges)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=ProductRelated_Ranges)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=Administrative_Duration_Ranges)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=Informational_Duration_Ranges)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=ProductRelated_Duration_Ranges)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(data, aes(x=Administrative_Duration_Ranges)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
seed = 123
set.seed(seed)
trainIndex <- createDataPartition(data$Revenue, p = .75, list = FALSE, times = 1)
seed = 123
set.seed(seed)
trainIndex <- createDataPartition(data$Revenue, p = .75, list = FALSE, times = 1)
dataTrain <- data[trainIndex,]
dataTest <-  data[-trainIndex,]
dim(dataTrain)
dim(dataTest)
trainControl=trainControl(method="cv", number=5)
set.seed(seed)
model_arbol1 <- train(Revenue~., dataTrain,
method='rpart',
trControl = trainControl)
model_arbol1 <- train(Revenue~., dataTrain,
method='rpart',
trControl = trainControl)
model_arbol1
plot(model_arbol1)
m <- model_arbol1$finalModel
rpart.plot(m, cex = 0.7)
m$parms
m$control
m$variable.importance
plot(m$variable.importance)
set.seed(seed) # el valor de la semilla, en sí, no es importante, solo que se utilice siempre el mismo
grid <- data.frame(cp=c(0)) #Fijamos en 0 el parámetro de complejidad eliminando la poda controlada por Caret
modelo_arbol2 <- train(Revenue~., data = dataTrain, method = "rpart",
tuneGrid=grid,
control = rpart.control(
minsplit = 2, #min instancias para intentar split de un nodo, por defecto es 20
minbucket = 1, #min instancias en un nodo hoja terminal, por defecto minsplit/3
maxdepth = 10 #max depth, por defecto es 30
))
modelo_arbol2 <- train(Revenue~., data = dataTrain, method = "rpart",
tuneGrid=grid,
control = rpart.control(
minsplit = 2, #min instancias para intentar split de un nodo, por defecto es 20
minbucket = 1, #min instancias en un nodo hoja terminal, por defecto minsplit/3
maxdepth = 10 #max depth, por defecto es 30
))
rpart.plot(modelo_arbol2$finalModel)
plot(model_arbol1)
rpart.plot(model_arbol1$finalModel)
m2 <- rpart(Revenue~., data = dataTrain,
control = rpart.control(
cp=0, # sin prepoda de complejidad
minsplit = 2, #min instancias para intentar split de un nodo, por defecto es 20
minbucket = 1, #min instancias en un nodo hoja terminal, por defecto minsplit/3
maxdepth = 10, #max depth, por defecto es 30
xval=10 # rpart hace validación cruzada sobre el set de entranamiento para estimar los errores relativos
))
head(m2$cptable, 20)
# Analicemos los valores del error relativo con respecto al valor de CP. Entre mayor el valor del CP, menor la complejidad del árbol
ggplot(data.frame(m2$cptable), aes(x=CP, y=rel.error)) +
geom_point(size=2) +
scale_x_continuous(limits = c(0, 0.25), breaks = seq(0,0.25, 0.02)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2))
plotcp(m2)
# Intervalo de error
i = which.min(m2$cptable[,4])
lim_inferior = m2$cptable[i,4] - m2$cptable[i,5]
lim_superior = m2$cptable[i,4] + m2$cptable[i,5]
print(paste("Intervalo de error: [", lim_inferior, ", ", lim_superior, "]"))
indices = which((m2$cptable[,4] >= lim_inferior) & (m2$cptable[,4] <= lim_superior))
indice_corte = min(indices)
umbral_corte_cp = m2$cptable[indice_corte,1]
umbral_corte_cp
m2 = prune(m2, cp=umbral_corte_cp)
m2$cptable
rpart.plot(m2)
predicciones <- predict(m2, newdata=dataTest, type='class')
confusionMatrix(predicciones, dataTest$Revenue)
predicciones <- predict(model_arbol1, newdata=dataTest, type='class')
predicciones <- predict(m1, newdata=dataTest, type='class')
rpart.plot(m)
predicciones <- predict(m, newdata=dataTest, type='class')
base2 = base
base2$Administrative_Ranges = NULL
base2$Informational_Ranges = NULL
base2$ProductRelated_Ranges = NULL
base2$Administrative_Duration_Ranges = NULL
base2$Informational_Duration_Ranges = NULL
base2$ProductRelated_Duration_Ranges = NULL
base2$ExitRates_Ranges = NULL
base2$BounceRates_Ranges = NULL
base2$PageValues_Ranges = NULL
seed = 123
set.seed(seed)
trainIndex <- createDataPartition(base2$Revenue, p = .75, list = FALSE, times = 1)
dataTrain <- base2[trainIndex,]
dataTest <-  base2[-trainIndex,]
dim(dataTrain)
dim(dataTest)
trainControl=trainControl(method="cv", number=5)
set.seed(seed)
model_arbol1 <- train(Revenue~., dataTrain,
method='rpart',
trControl = trainControl)
model_arbol1
plot(model_arbol1)
m <- model_arbol1$finalModel
rpart.plot(m, cex = 0.7)
m$parms
m$control
m$variable.importance
plot(m$variable.importance)
############################################################################
# Modelo sin poda                                                          #
############################################################################
set.seed(seed) # el valor de la semilla, en sí, no es importante, solo que se utilice siempre el mismo
grid <- data.frame(cp=c(0)) #Fijamos en 0 el parámetro de complejidad eliminando la poda controlada por Caret
modelo_arbol2 <- train(Revenue~., data = dataTrain, method = "rpart",
tuneGrid=grid,
control = rpart.control(
minsplit = 2, #min instancias para intentar split de un nodo, por defecto es 20
minbucket = 1, #min instancias en un nodo hoja terminal, por defecto minsplit/3
maxdepth = 10 #max depth, por defecto es 30
))
rpart.plot(modelo_arbol2$finalModel)
############################################################################
# Modelo con post-poda                                                     #
############################################################################
set.seed(seed) # el valor de la semilla, en sí, no es importante, solo que se utilice siempre el mismo
m2 <- rpart(Revenue~., data = dataTrain,
control = rpart.control(
cp=0, # sin prepoda de complejidad
minsplit = 2, #min instancias para intentar split de un nodo, por defecto es 20
minbucket = 1, #min instancias en un nodo hoja terminal, por defecto minsplit/3
maxdepth = 10, #max depth, por defecto es 30
xval=10 # rpart hace validación cruzada sobre el set de entranamiento para estimar los errores relativos
))
head(m2$cptable, 20)
# Analicemos los valores del error relativo con respecto al valor de CP. Entre mayor el valor del CP, menor la complejidad del árbol
ggplot(data.frame(m2$cptable), aes(x=CP, y=rel.error)) +
geom_point(size=2) +
scale_x_continuous(limits = c(0, 0.25), breaks = seq(0,0.25, 0.02)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2))
plotcp(m2)
# Intervalo de error
i = which.min(m2$cptable[,4])
lim_inferior = m2$cptable[i,4] - m2$cptable[i,5]
lim_superior = m2$cptable[i,4] + m2$cptable[i,5]
print(paste("Intervalo de error: [", lim_inferior, ", ", lim_superior, "]"))
indices = which((m2$cptable[,4] >= lim_inferior) & (m2$cptable[,4] <= lim_superior))
indice_corte = min(indices)
umbral_corte_cp = m2$cptable[indice_corte,1]
umbral_corte_cp
m2 = prune(m2, cp=umbral_corte_cp)
m2$cptable
rpart.plot(m2)
predicciones <- predict(m2, newdata=dataTest, type='class')
confusionMatrix(predicciones, dataTest$Revenue)
table(data$Revenue)
cat(sprintf("htmlesc<<p,Proporciones de compra (Revenue):>>\n"))
print(prop.table(table(data$Revenue)))
cat(sprintf("htmlesc<<p,De acuerdo al comportamiento de los seguientes gráficos, podemos ver un comportamiento homogéneo. Sin embargo, existen conductas que valen la pena resaltar como:
- Los últimos 6 meses parecen influir en la intención de compra.
- Las visitas a páginas administrativas presentan un leve impacto positivo en la intención de compra, y mientras mas tiempo pasen, podría impactar de mejor forma.
- Las visitas a páginas informativas no parecen tener un impacto representativo en la intención de compra. Sin embargo, los tiempos de la visita impactan positivamente.
- Las visitas a páginas relacionadas con el producto parecen tener un impacto negativo sobre las compras. Sin embargo, si la visita dura mas de 10 minutos, el impacto podría ser áltamente positivo.
- Las mediciones de salida son en mayor proporción negativas, aunque en un sólo caso parece impactar la intención de compra en gran medida (<= 0.05).
- A mayor taza de rebote, disminuye la intención de compra.
- Parece ser que los valores de las páginas (PageValues) son la variable que mayor impacto positivo tiene en la compra, ya que sólo en el caso en que su valor sea 0 o menor, no se realizan compras. Mientras que si su valor aumenta, hay un inmenente crecimiento en las compras.
- El resto de las variables no generan ningún impacto representativo.
- >>\n"))
ggplot(base, aes(x=Month)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=OperatingSystems)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Browser)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Region)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=TrafficType)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=VisitorType)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Administrative)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Informational)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=ProductRelated)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Administrative_Duration)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Informational_Duration)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=ProductRelated_Duration)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=ExitRates)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=BounceRates)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=PageValues)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Weekend)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=SpecialDay)) + geom_density(aes(group=Revenue, colour=Revenue, fill=Revenue), alpha=0.3)
ggplot(base, aes(x=Administrative , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Informational , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=ProductRelated , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Administrative_Duration , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=Informational_Duration , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=ProductRelated_Duration , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=ExitRates , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
ggplot(base, aes(x=BounceRates , y=PageValues, color=Revenue)) + geom_point(size=2, alpha=.5)
cat(sprintf("htmlesc<<h5,Entrenamiento del modelo>>"))
cat(sprintf("htmlesc<<p,Condiciones:
- Datos de entrenamiento: 75%
- Datos de prueba: 25%
- Método de entrenamiento: 5 Cross Validation>>"))
cat(sprintf("htmlesc<<p,Condiciones:
- Datos de entrenamiento: 75%
- Datos de prueba: 25%
- Método de entrenamiento: 5 Cross Validation>>"))
cat(sprintf("htmlesc<<h5,Entrenamiento del modelo\n>>"))
cat(sprintf("htmlesc<<p,Condiciones:\n- Datos de entrenamiento: 75%\n- Datos de prueba: 25%\n- Método de entrenamiento: 5 Cross Validation\n>>"))
cat(sprintf("htmlesc<<p,Condiciones:
- Datos de entrenamiento: 75\%
cat(sprintf("htmlesc<<p,Condiciones:
- Datos de entrenamiento: 75\\%
- Datos de prueba: 25\\%
- Método de entrenamiento: 5 Cross Validation>>"))
cat(sprintf("htmlesc<<p,Condiciones:
- Datos de entrenamiento: 75 perc
- Datos de prueba: 25 perc
- Método de entrenamiento: 5 Cross Validation>>"))
base2 = base
base2 = base
base2$Administrative_Ranges = NULL
base2$Informational_Ranges = NULL
base2$ProductRelated_Ranges = NULL
base2$Administrative_Duration_Ranges = NULL
base2$Informational_Duration_Ranges = NULL
base2$ProductRelated_Duration_Ranges = NULL
base2$ExitRates_Ranges = NULL
base2$BounceRates_Ranges = NULL
base2$PageValues_Ranges = NULL
seed = 123
set.seed(seed)
trainIndex <- createDataPartition(base2$Revenue, p = .75, list = FALSE, times = 1)
dataTrain <- base2[trainIndex,]
dataTest <-  base2[-trainIndex,]
dim(dataTrain)
dim(dataTest)
trainControl=trainControl(method="cv", number=5)
set.seed(seed)
model_arbol1 <- train(Revenue~., dataTrain,
method='rpart',
trControl = trainControl)
model_arbol1 <- train(Revenue~., dataTrain,
method='rpart',
trControl = trainControl)
model_arbol1
plot(model_arbol1)
m <- model_arbol1$finalModel
rpart.plot(m, cex = 0.7)
m$parms
m$control
plot(m$variable.importance)
############################################################################
# Modelo sin poda                                                          #
############################################################################
cat(sprintf("htmlesc<<h5,Validación del modelo - Sin poda>>"))
rpart.plot(model_arbol1$finalModel)
############################################################################
# Modelo con post-poda                                                     #
############################################################################
cat(sprintf("htmlesc<<h5,Validación del modelo - Con poda>>"))
set.seed(seed) # el valor de la semilla, en sí, no es importante, solo que se utilice siempre el mismo
m2 <- rpart(Revenue~., data = dataTrain,
control = rpart.control(
cp=0, # sin prepoda de complejidad
minsplit = 2, #min instancias para intentar split de un nodo, por defecto es 20
minbucket = 1, #min instancias en un nodo hoja terminal, por defecto minsplit/3
maxdepth = 10, #max depth, por defecto es 30
xval=10 # rpart hace validación cruzada sobre el set de entranamiento para estimar los errores relativos
))
head(m2$cptable, 20)
plotcp(m2)
# Intervalo de error
i = which.min(m2$cptable[,4])
lim_inferior = m2$cptable[i,4] - m2$cptable[i,5]
lim_superior = m2$cptable[i,4] + m2$cptable[i,5]
print(paste("Intervalo de error: [", lim_inferior, ", ", lim_superior, "]"))
indices = which((m2$cptable[,4] >= lim_inferior) & (m2$cptable[,4] <= lim_superior))
indice_corte = min(indices)
umbral_corte_cp = m2$cptable[indice_corte,1]
umbral_corte_cp
m2 = prune(m2, cp=umbral_corte_cp)
m2$cptable
rpart.plot(m2)
predicciones <- predict(m2, newdata=dataTest, type='class')
confusionMatrix(predicciones, dataTest$Revenue)
library(readxl)
datos = read_excel("datosExcel.xlsx",col_types = c("date", "numeric", "numeric", "numeric", "numeric"))
datos = read_excel("datosEmpleo.xlsx",col_types = c("date", "numeric", "numeric", "numeric", "numeric"))
datos = read_excel('datosEmpleo.xlsx',col_types = c("date", "numeric", "numeric", "numeric", "numeric"))
library(readxl)
library(readxl)
data <-read_excel("datosEmpleo.xlsx", col_types = c("date", "numeric", "numeric", "numeric", "numeric") )
data <-read_excel("datosEmpleo.xlsx", col_types = c("date", "numeric", "numeric", "numeric", "numeric") )
knitr::opts_chunk$set(echo = TRUE)
ARMAacf(ar = c(0.25), ma=0, lag.max = 25, pacf = FALSE)
r1<- ARMAacf(ar =0 , ma=c(0.5,0.2), lag.max = 25, pacf = FALSE)
plot(r1, type="h", col="blue", xlab=expression(s),
ylab=expression(gamma[s]),
main = expression(paste(group("(",list( theta[1],
theta[2]), ")"),"=",
group("(",list( .5, .2), ")"))))
abline(h=0)
plot(r1, type="h", col="blue", xlab=expression(s),
ylab=expression(gamma[s]),
main = expression(paste(group("(",list( theta[1],
theta[2]), ")"),"=",
group("(",list( .5, .2), ")"))))
plot(r1, type="h", col="blue", xlab=expression(s),
ylab=expression(gamma[s]),
main = expression(paste(group("(",list( theta[1],
theta[2]), ")"),"=",
group("(",list( .5, .2), ")"))))+
abline(h=0)
plot(r1, type="h", col="blue", xlab=expression(s),
ylab=expression(gamma[s]),
main = expression(paste(group("(",list( theta[1],
theta[2]), ")"),"=",
group("(",list( .5, .2), ")"))))+
abline(h=0)
install.packages(c("astsa", "ggpubr"))
install.packages(c("astsa", "ggpubr"))
```{r setup, include=FALSE}
library(ggplot2)
library(caret)
library(corrplot)
library(fpc)
install.packages('fpc')
install.packages('cluster')
install.packages('factoextra')
install.packages("factoextra")
library(factoextra)
install.packages('factoextra')
install.packages("factoextra")
install.packages("factoextra")
install.packages('factoextra')
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
knitr::opts_chunk$set(echo = TRUE)
#install.packages('clustMixType')
library(clustMixType)
n <- 100
prb <- 0.9
muk <- 1.5
clusid <- rep(1:4, each = n)
x1 <- sample(c("A","B"), 2*n, replace = TRUE, prob = c(prb, 1-prb))
x1 <- c(x1, sample(c("A","B"), 2*n, replace = TRUE, prob = c(1-prb, prb)))
x1 <- as.factor(x1)
x2 <- sample(c("A","B"), 2*n, replace = TRUE, prob = c(prb, 1-prb))
x2 <- c(x2, sample(c("A","B"), 2*n, replace = TRUE, prob = c(1-prb, prb)))
x2 <- as.factor(x2)
x3 <- c(rnorm(n, mean = -muk), rnorm(n, mean = muk), rnorm(n, mean = -muk), rnorm(n, mean = muk))
x4 <- c(rnorm(n, mean = -muk), rnorm(n, mean = muk), rnorm(n, mean = -muk), rnorm(n, mean = muk))
x <- data.frame(x1,x2,x3,x4)
x
kpres <- kproto(x, 4)
kpres
clprofiles(kpres, x)
